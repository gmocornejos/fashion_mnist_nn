{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa85d055-6560-4181-b559-7810c2962476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d52d7e6-3722-4cfd-aa35-570eb5efccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/train_data.pkl\", \"rb\") as train_file:\n",
    "    train_data = pickle.load(train_file)\n",
    "\n",
    "with open(\"data/test_data.pkl\", \"rb\") as test_file:\n",
    "    test_data = pickle.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ef8a735-6896-4a23-9fe1-115f0eb9a99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network\n",
    "\n",
    "img_px = 28\n",
    "n_i = 512\n",
    "n_1 = 128\n",
    "n_2 = 64\n",
    "n_L = 10 # there are 10 classes in FashionMNIST\n",
    "\n",
    "k_i = 1/np.sqrt( img_px * img_px )\n",
    "k_1 = 1/np.sqrt( n_i * n_i )\n",
    "k_2 = 1/np.sqrt( n_1 * n_1)\n",
    "k_L = 1/np.sqrt( n_2 * n_2 )\n",
    "\n",
    "w_i = rng.uniform( -k_i, k_i, (n_i, img_px*img_px) )\n",
    "b_i = rng.normal( -k_i, k_i, size=n_i )\n",
    "\n",
    "w_1 = rng.normal( -k_1, k_1, size=(n_1, n_i) )\n",
    "b_1 = rng.normal( -k_1, k_1, size=n_1 )\n",
    "\n",
    "w_2 = rng.normal( -k_2, k_2, size=(n_2, n_1))\n",
    "b_2 = rng.normal( -k_2, k_2, size=n_2 )\n",
    "\n",
    "w_L = rng.normal( -k_L, k_L, size=(n_L, n_2) )\n",
    "b_L = rng.normal( -k_L, k_L, size=n_L )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4cfbfd-75e2-4cba-b0f0-e78dbbb96224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1004151 , 0.09923117, 0.10055567, 0.10118387, 0.09933581,\n",
       "       0.10178994, 0.09841747, 0.0987981 , 0.10017248, 0.10010039])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward function, used to make predictions\n",
    "\n",
    "def forward(x, return_z_a=False):\n",
    "    x = x.flatten()\n",
    "    \n",
    "    z_i = w_i @ x + b_i\n",
    "    a_i = np.maximum(0, z_i)\n",
    "\n",
    "    z_1 = w_1 @ a_i + b_1\n",
    "    a_1 = np.maximum(0, z_1)\n",
    "\n",
    "    z_2 = w_2 @ a_1 + b_2\n",
    "    a_2 = np.maximum(0, z_2)\n",
    "\n",
    "    z_L = w_L @ a_2 + b_L\n",
    "    a_L = softmax(z_L)\n",
    "\n",
    "    if return_z_a:\n",
    "        return (z_i, z_1, z_2, z_L), (a_i, a_1, a_2, a_L)\n",
    "    else:\n",
    "        return a_L\n",
    "\n",
    "forward(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c13a1b2-0959-4e7d-9bdf-a7fe5f3a0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1, 2.302635596332296)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross-entropy loss\n",
    "\n",
    "def loss_fn(y, a_L):\n",
    "    return - (y * np.log(a_L)).sum()   \n",
    "\n",
    "\n",
    "def test():\n",
    "    n_samples = len(test_data)\n",
    "    correct = 0\n",
    "    sum_loss = 0\n",
    "    for x, y in test_data:\n",
    "        y_hat = forward(x)\n",
    "        correct += y.argmax() == y_hat.argmax()\n",
    "        sum_loss += loss_fn(y, y_hat)\n",
    "    return correct / n_samples, sum_loss / n_samples        \n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3124b8-22cd-438d-94dd-58c9e2caf634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_derivative(x):\n",
    "    r = np.ones(x.shape)\n",
    "    r[x == 0] = 0\n",
    "    return r\n",
    "\n",
    "def calc_derivatives(x, y):\n",
    "    x = x.flatten()\n",
    "    \n",
    "    (z_i, z_1, z_2, z_L), (a_i, a_1, a_2, a_L) = forward(x, True)\n",
    "    \n",
    "    nabla_z_L = a_L - y\n",
    "    nabla_w_L = np.outer(nabla_z_L, a_2)\n",
    "    \n",
    "    nabla_z_2 = ReLU_derivative(z_2) * (w_L.T @ nabla_z_L)\n",
    "    nabla_w_2 = np.outer(nabla_z_2, a_1)\n",
    "    \n",
    "    nabla_z_1 = ReLU_derivative(z_1) * (w_2.T @ nabla_z_2)\n",
    "    nabla_w_1 = np.outer(nabla_z_1, a_i)\n",
    "    \n",
    "    nabla_z_i = ReLU_derivative(z_i) * (w_1.T @ nabla_z_1)\n",
    "    nabla_w_i = np.outer(nabla_z_i, x)\n",
    "\n",
    "    return (nabla_w_i, nabla_w_1, nabla_w_2, nabla_w_L), (nabla_z_i, nabla_z_1, nabla_z_2, nabla_z_L), loss_fn(y, a_L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56a18c46-5ec1-474c-ba45-35f7a46cf21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train stats\n",
      "==> Accuracy: 10.0%, Avg loss: 2.302636\n",
      "\n",
      "Epoch 0\n",
      "--------------------\n",
      "loss: 2.300198 [mini-batch 0 / 937]\n",
      "loss: 2.301184 [mini-batch 100 / 937]\n",
      "loss: 2.301416 [mini-batch 200 / 937]\n",
      "loss: 2.257401 [mini-batch 300 / 937]\n",
      "loss: 1.650731 [mini-batch 400 / 937]\n",
      "loss: 1.316656 [mini-batch 500 / 937]\n",
      "loss: 1.239089 [mini-batch 600 / 937]\n",
      "loss: 1.042560 [mini-batch 700 / 937]\n",
      "loss: 1.082339 [mini-batch 800 / 937]\n",
      "loss: 0.960052 [mini-batch 900 / 937]\n",
      "==> Accuracy: 59.2%, Avg loss: 0.970282\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 0.985221 [mini-batch 0 / 937]\n",
      "loss: 0.991065 [mini-batch 100 / 937]\n",
      "loss: 0.754201 [mini-batch 200 / 937]\n",
      "loss: 0.936900 [mini-batch 300 / 937]\n",
      "loss: 0.799626 [mini-batch 400 / 937]\n",
      "loss: 0.675703 [mini-batch 500 / 937]\n",
      "loss: 0.836319 [mini-batch 600 / 937]\n",
      "loss: 0.749523 [mini-batch 700 / 937]\n",
      "loss: 0.807046 [mini-batch 800 / 937]\n",
      "loss: 0.722393 [mini-batch 900 / 937]\n",
      "==> Accuracy: 72.7%, Avg loss: 0.716839\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 0.633655 [mini-batch 0 / 937]\n",
      "loss: 0.703740 [mini-batch 100 / 937]\n",
      "loss: 0.480426 [mini-batch 200 / 937]\n",
      "loss: 0.700687 [mini-batch 300 / 937]\n",
      "loss: 0.763599 [mini-batch 400 / 937]\n",
      "loss: 0.526624 [mini-batch 500 / 937]\n",
      "loss: 0.614304 [mini-batch 600 / 937]\n",
      "loss: 0.705184 [mini-batch 700 / 937]\n",
      "loss: 0.756670 [mini-batch 800 / 937]\n",
      "loss: 0.583075 [mini-batch 900 / 937]\n",
      "==> Accuracy: 78.0%, Avg loss: 0.609042\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 0.534362 [mini-batch 0 / 937]\n",
      "loss: 0.564146 [mini-batch 100 / 937]\n",
      "loss: 0.374936 [mini-batch 200 / 937]\n",
      "loss: 0.568975 [mini-batch 300 / 937]\n",
      "loss: 0.681895 [mini-batch 400 / 937]\n",
      "loss: 0.478367 [mini-batch 500 / 937]\n",
      "loss: 0.572509 [mini-batch 600 / 937]\n",
      "loss: 0.704054 [mini-batch 700 / 937]\n",
      "loss: 0.741282 [mini-batch 800 / 937]\n",
      "loss: 0.484427 [mini-batch 900 / 937]\n",
      "==> Accuracy: 79.6%, Avg loss: 0.586848\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 0.483912 [mini-batch 0 / 937]\n",
      "loss: 0.531675 [mini-batch 100 / 937]\n",
      "loss: 0.354530 [mini-batch 200 / 937]\n",
      "loss: 0.559605 [mini-batch 300 / 937]\n",
      "loss: 0.651449 [mini-batch 400 / 937]\n",
      "loss: 0.504108 [mini-batch 500 / 937]\n",
      "loss: 0.499651 [mini-batch 600 / 937]\n",
      "loss: 0.717840 [mini-batch 700 / 937]\n",
      "loss: 0.740733 [mini-batch 800 / 937]\n",
      "loss: 0.447626 [mini-batch 900 / 937]\n",
      "==> Accuracy: 81.0%, Avg loss: 0.544872\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 0.415720 [mini-batch 0 / 937]\n",
      "loss: 0.523978 [mini-batch 100 / 937]\n",
      "loss: 0.314835 [mini-batch 200 / 937]\n",
      "loss: 0.533083 [mini-batch 300 / 937]\n",
      "loss: 0.566871 [mini-batch 400 / 937]\n",
      "loss: 0.465120 [mini-batch 500 / 937]\n",
      "loss: 0.461061 [mini-batch 600 / 937]\n",
      "loss: 0.650488 [mini-batch 700 / 937]\n",
      "loss: 0.743773 [mini-batch 800 / 937]\n",
      "loss: 0.432278 [mini-batch 900 / 937]\n",
      "==> Accuracy: 81.6%, Avg loss: 0.525024\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 0.391049 [mini-batch 0 / 937]\n",
      "loss: 0.510106 [mini-batch 100 / 937]\n",
      "loss: 0.289994 [mini-batch 200 / 937]\n",
      "loss: 0.514924 [mini-batch 300 / 937]\n",
      "loss: 0.513975 [mini-batch 400 / 937]\n",
      "loss: 0.432687 [mini-batch 500 / 937]\n",
      "loss: 0.441682 [mini-batch 600 / 937]\n",
      "loss: 0.590048 [mini-batch 700 / 937]\n",
      "loss: 0.760563 [mini-batch 800 / 937]\n",
      "loss: 0.430107 [mini-batch 900 / 937]\n",
      "==> Accuracy: 81.8%, Avg loss: 0.517195\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 0.369697 [mini-batch 0 / 937]\n",
      "loss: 0.516143 [mini-batch 100 / 937]\n",
      "loss: 0.281398 [mini-batch 200 / 937]\n",
      "loss: 0.493009 [mini-batch 300 / 937]\n",
      "loss: 0.489857 [mini-batch 400 / 937]\n",
      "loss: 0.420998 [mini-batch 500 / 937]\n",
      "loss: 0.435071 [mini-batch 600 / 937]\n",
      "loss: 0.559360 [mini-batch 700 / 937]\n",
      "loss: 0.749013 [mini-batch 800 / 937]\n",
      "loss: 0.420592 [mini-batch 900 / 937]\n",
      "==> Accuracy: 81.6%, Avg loss: 0.515930\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 0.368518 [mini-batch 0 / 937]\n",
      "loss: 0.507703 [mini-batch 100 / 937]\n",
      "loss: 0.284263 [mini-batch 200 / 937]\n",
      "loss: 0.475690 [mini-batch 300 / 937]\n",
      "loss: 0.491881 [mini-batch 400 / 937]\n",
      "loss: 0.421773 [mini-batch 500 / 937]\n",
      "loss: 0.423625 [mini-batch 600 / 937]\n",
      "loss: 0.524305 [mini-batch 700 / 937]\n",
      "loss: 0.701027 [mini-batch 800 / 937]\n",
      "loss: 0.423450 [mini-batch 900 / 937]\n",
      "==> Accuracy: 81.7%, Avg loss: 0.516894\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 0.370351 [mini-batch 0 / 937]\n",
      "loss: 0.484902 [mini-batch 100 / 937]\n",
      "loss: 0.285779 [mini-batch 200 / 937]\n",
      "loss: 0.485696 [mini-batch 300 / 937]\n",
      "loss: 0.500473 [mini-batch 400 / 937]\n",
      "loss: 0.424122 [mini-batch 500 / 937]\n",
      "loss: 0.404956 [mini-batch 600 / 937]\n",
      "loss: 0.506667 [mini-batch 700 / 937]\n",
      "loss: 0.655201 [mini-batch 800 / 937]\n",
      "loss: 0.419677 [mini-batch 900 / 937]\n",
      "==> Accuracy: 81.5%, Avg loss: 0.523127\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 0.376938 [mini-batch 0 / 937]\n",
      "loss: 0.481801 [mini-batch 100 / 937]\n",
      "loss: 0.329051 [mini-batch 200 / 937]\n",
      "loss: 0.515182 [mini-batch 300 / 937]\n",
      "loss: 0.542303 [mini-batch 400 / 937]\n",
      "loss: 0.462079 [mini-batch 500 / 937]\n",
      "loss: 0.413545 [mini-batch 600 / 937]\n",
      "loss: 0.519299 [mini-batch 700 / 937]\n",
      "loss: 0.640101 [mini-batch 800 / 937]\n",
      "loss: 0.497947 [mini-batch 900 / 937]\n",
      "==> Accuracy: 79.6%, Avg loss: 0.580803\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "loss: 0.422560 [mini-batch 0 / 937]\n",
      "loss: 0.485703 [mini-batch 100 / 937]\n",
      "loss: 2.067476 [mini-batch 200 / 937]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22102/2596950636.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return - (y * np.log(a_L)).sum()\n",
      "/tmp/ipykernel_22102/2596950636.py:4: RuntimeWarning: invalid value encountered in multiply\n",
      "  return - (y * np.log(a_L)).sum()\n",
      "/tmp/ipykernel_22102/1772072483.py:9: RuntimeWarning: overflow encountered in matmul\n",
      "  z_1 = w_1 @ a_i + b_1\n",
      "/tmp/ipykernel_22102/1772072483.py:12: RuntimeWarning: invalid value encountered in matmul\n",
      "  z_2 = w_2 @ a_1 + b_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 937]\n",
      "loss:      nan [mini-batch 100 / 937]\n",
      "loss:      nan [mini-batch 200 / 937]\n",
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 937]\n",
      "loss:      nan [mini-batch 100 / 937]\n",
      "loss:      nan [mini-batch 200 / 937]\n",
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 937]\n",
      "loss:      nan [mini-batch 100 / 937]\n",
      "loss:      nan [mini-batch 200 / 937]\n",
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 937]\n",
      "loss:      nan [mini-batch 100 / 937]\n",
      "loss:      nan [mini-batch 200 / 937]\n",
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 937]\n",
      "loss:      nan [mini-batch 100 / 937]\n",
      "loss:      nan [mini-batch 200 / 937]\n",
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 937]\n",
      "loss:      nan [mini-batch 100 / 937]\n",
      "loss:      nan [mini-batch 200 / 937]\n",
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 937]\n",
      "loss:      nan [mini-batch 100 / 937]\n",
      "loss:      nan [mini-batch 200 / 937]\n",
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "loss:      nan [mini-batch 0 / 937]\n",
      "loss:      nan [mini-batch 100 / 937]\n",
      "loss:      nan [mini-batch 200 / 937]\n",
      "loss:      nan [mini-batch 300 / 937]\n",
      "loss:      nan [mini-batch 400 / 937]\n",
      "loss:      nan [mini-batch 500 / 937]\n",
      "loss:      nan [mini-batch 600 / 937]\n",
      "loss:      nan [mini-batch 700 / 937]\n",
      "loss:      nan [mini-batch 800 / 937]\n",
      "loss:      nan [mini-batch 900 / 937]\n",
      "==> Accuracy: 10.0%, Avg loss:      nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train loop\n",
    "\n",
    "learning_rate = 1e-3\n",
    "mini_batch_size = 64\n",
    "training_epochs = 5\n",
    "\n",
    "n_mini_batches = len(train_data) // mini_batch_size\n",
    "training_index = np.arange(len(train_data))\n",
    "\n",
    "print(\"Pre-train stats\")\n",
    "accuracy, avg_loss = test()\n",
    "print(f\"==> Accuracy: {100*accuracy:0.1f}%, Avg loss: {avg_loss:>8f}\\n\")\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    print(f\"Epoch {epoch}\\n\" + 20*'-')\n",
    "    rng.shuffle(training_index)\n",
    "    for mini_batch_number in range(n_mini_batches):\n",
    "        # batch average derivatives\n",
    "        sum_nabla_w_i = np.zeros(w_i.shape)\n",
    "        sum_nabla_w_1 = np.zeros(w_1.shape)\n",
    "        sum_nabla_w_2 = np.zeros(w_2.shape)\n",
    "        sum_nabla_w_L = np.zeros(w_L.shape)\n",
    "        sum_nabla_b_i = np.zeros(b_i.shape)\n",
    "        sum_nabla_b_1 = np.zeros(b_1.shape)\n",
    "        sum_nabla_b_2 = np.zeros(b_2.shape)\n",
    "        sum_nabla_b_L = np.zeros(b_L.shape)\n",
    "        sum_loss = 0\n",
    "        mini_batch = train_data[mini_batch_number*mini_batch_size : (mini_batch_number+1)*mini_batch_size]\n",
    "        for x, y in mini_batch:\n",
    "            (nabla_w_i, nabla_w_1, nabla_w_2, nabla_w_L), (nabla_b_i, nabla_b_1, nabla_b_2, nabla_b_L), loss = calc_derivatives(x, y)\n",
    "            sum_nabla_w_i += nabla_w_i\n",
    "            sum_nabla_w_1 += nabla_w_1\n",
    "            sum_nabla_w_2 += nabla_w_2\n",
    "            sum_nabla_w_L += nabla_w_L\n",
    "            sum_nabla_b_i += nabla_b_i\n",
    "            sum_nabla_b_1 += nabla_b_1\n",
    "            sum_nabla_b_2 += nabla_b_2\n",
    "            sum_nabla_b_L += nabla_b_L\n",
    "            sum_loss += loss\n",
    "        w_i -= learning_rate * (sum_nabla_w_i / len(mini_batch))\n",
    "        w_1 -= learning_rate * (sum_nabla_w_1 / len(mini_batch))\n",
    "        w_2 -= learning_rate * (sum_nabla_w_2 / len(mini_batch))\n",
    "        w_L -= learning_rate * (sum_nabla_w_L / len(mini_batch))\n",
    "        b_i -= learning_rate * (sum_nabla_b_i / len(mini_batch))\n",
    "        b_1 -= learning_rate * (sum_nabla_b_1 / len(mini_batch))\n",
    "        b_2 -= learning_rate * (sum_nabla_b_2 / len(mini_batch))\n",
    "        b_L -= learning_rate * (sum_nabla_b_L / len(mini_batch))\n",
    "        if mini_batch_number % 100 == 0:\n",
    "            print(f\"loss: {sum_loss/len(mini_batch):>8f} [mini-batch {mini_batch_number} / {n_mini_batches}]\")\n",
    "    # Test at the end of each epoch\n",
    "    accuracy, avg_loss = test()\n",
    "    print(f\"==> Accuracy: {100*accuracy:0.1f}%, Avg loss: {avg_loss:>8f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
