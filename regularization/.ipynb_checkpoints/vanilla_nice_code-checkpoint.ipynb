{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "269f83d4-70d4-40dc-b3d3-2a99c6223eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "with open(\"../data/train_data.pkl\", \"rb\") as train_file:\n",
    "    train_data = pickle.load(train_file)\n",
    "\n",
    "with open(\"../data/test_data.pkl\", \"rb\") as test_file:\n",
    "    test_data = pickle.load(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c82f2b8b-eda2-4557-8035-3bcd47af6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_def = [28*28, 512, 128, 64, 10]\n",
    "num_layers = len(network_def) - 1\n",
    "\n",
    "# Create network layers\n",
    "weights = []\n",
    "biases = []\n",
    "for l in range(num_layers):\n",
    "    k = np.sqrt(1 / network_def[l])\n",
    "    weights.append( rng.uniform(-k, k, size=(network_def[l+1], network_def[l])) )\n",
    "    biases.append( rng.uniform(-k, k, size=network_def[l+1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b226b2fd-1dbf-4f86-9563-3a8851de4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, return_z_a=False):\n",
    "    z = [0 for l in range(num_layers)]\n",
    "    a = [0 for l in range(num_layers)]\n",
    "\n",
    "    # First layer\n",
    "    l = 0\n",
    "    z[l] = weights[l] @ x.flatten() + biases[l]\n",
    "    a[l] = np.maximum(0, z[l])\n",
    "    # Hidden layers\n",
    "    for l in range(1, num_layers-1):\n",
    "        z[l] = weights[l] @ a[l-1] + biases[l]\n",
    "        a[l] = np.maximum(0, z[l])\n",
    "    # Last layer\n",
    "    l = num_layers-1\n",
    "    z[l] = weights[l] @ a[l-1] + biases[l]\n",
    "    a[l] = softmax(z[l])\n",
    "            \n",
    "    if return_z_a:\n",
    "        return z, a\n",
    "    else:\n",
    "        return a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cd24a0e-a0f6-4671-a10b-e33c1ed14797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y, y_hat):\n",
    "    return - (y * np.log(y_hat)).sum()   \n",
    "\n",
    "\n",
    "def test():\n",
    "    n_samples = len(test_data)\n",
    "    correct = 0\n",
    "    sum_loss = 0\n",
    "    for x, y in test_data:\n",
    "        y_hat = forward(x)\n",
    "        correct += y.argmax() == y_hat.argmax()\n",
    "        sum_loss += loss_fn(y, y_hat)\n",
    "    return correct / n_samples, sum_loss / n_samples        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef428824-8e2b-45d4-92bb-178d4b6fd12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_derivative(x):\n",
    "    r = np.ones(x.shape)\n",
    "    r[x == 0] = 0\n",
    "    return r\n",
    "\n",
    "def calc_gradient(x, y):\n",
    "    x = x.flatten()\n",
    "    z, a = forward(x, True)\n",
    "\n",
    "    nabla_b = [0 for l in range(num_layers)]\n",
    "    nabla_w = [0 for l in range(num_layers)]\n",
    "    # Last layer\n",
    "    l = num_layers -1\n",
    "    nabla_b[l] = a[l] - y\n",
    "    nabla_w[l] = np.outer(nabla_b[l], a[l-1])\n",
    "    # All other layers\n",
    "    for i in range(1, num_layers-1):\n",
    "        l = (num_layers - 1) - i\n",
    "        nabla_b[l] = ReLU_derivative(z[l]) * (weights[l+1].T @ nabla_b[l+1])\n",
    "        nabla_w[l] = np.outer(nabla_b[l], a[l-1])\n",
    "    # First layer\n",
    "    l = 0\n",
    "    nabla_b[l] = ReLU_derivative(z[l]) * (weights[l+1].T @ nabla_b[l+1])\n",
    "    nabla_w[l] = np.outer(nabla_b[l], x)\n",
    "\n",
    "    return nabla_b, nabla_w, loss_fn(y, a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b21a675c-9367-44ee-990d-0c671b3594bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "\n",
    "learning_rate = 1e-3\n",
    "mini_batch_size = 64\n",
    "training_epochs = 20\n",
    "\n",
    "n_mini_batches = len(train_data) // mini_batch_size\n",
    "training_index = np.arange(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dfbfc4f-73a2-46b2-9dab-3c7ed71dd429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-train stats\n",
      "==> Accuracy: 15.0%, Avg loss: 5.379647\n",
      "\n",
      "Epoch 0\n",
      "--------------------\n",
      "loss: 27.084501 [mini-batch 0 / 937]\n",
      "loss: 3.617760 [mini-batch 100 / 937]\n",
      "loss: 2.117557 [mini-batch 200 / 937]\n",
      "loss: 2.455038 [mini-batch 300 / 937]\n",
      "loss: 2.575025 [mini-batch 400 / 937]\n",
      "loss: 2.375221 [mini-batch 500 / 937]\n",
      "loss: 2.269079 [mini-batch 600 / 937]\n",
      "loss: 2.231560 [mini-batch 700 / 937]\n",
      "loss: 2.535130 [mini-batch 800 / 937]\n",
      "loss: 1.635679 [mini-batch 900 / 937]\n",
      "==> Accuracy: 80.9%, Avg loss: 0.559572\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "loss: 1.996562 [mini-batch 0 / 937]\n",
      "loss: 2.176700 [mini-batch 100 / 937]\n",
      "loss: 1.608836 [mini-batch 200 / 937]\n",
      "loss: 1.950641 [mini-batch 300 / 937]\n",
      "loss: 2.270520 [mini-batch 400 / 937]\n",
      "loss: 1.916250 [mini-batch 500 / 937]\n",
      "loss: 1.924212 [mini-batch 600 / 937]\n",
      "loss: 2.015887 [mini-batch 700 / 937]\n",
      "loss: 2.207054 [mini-batch 800 / 937]\n",
      "loss: 1.626994 [mini-batch 900 / 937]\n",
      "==> Accuracy: 82.2%, Avg loss: 0.519820\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "loss: 1.755879 [mini-batch 0 / 937]\n",
      "loss: 2.043933 [mini-batch 100 / 937]\n",
      "loss: 1.581420 [mini-batch 200 / 937]\n",
      "loss: 1.905809 [mini-batch 300 / 937]\n",
      "loss: 2.240904 [mini-batch 400 / 937]\n",
      "loss: 1.762460 [mini-batch 500 / 937]\n",
      "loss: 1.864280 [mini-batch 600 / 937]\n",
      "loss: 2.045186 [mini-batch 700 / 937]\n",
      "loss: 2.094944 [mini-batch 800 / 937]\n",
      "loss: 1.669489 [mini-batch 900 / 937]\n",
      "==> Accuracy: 82.8%, Avg loss: 0.505644\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "loss: 1.610553 [mini-batch 0 / 937]\n",
      "loss: 1.937397 [mini-batch 100 / 937]\n",
      "loss: 1.471229 [mini-batch 200 / 937]\n",
      "loss: 1.860432 [mini-batch 300 / 937]\n",
      "loss: 2.189515 [mini-batch 400 / 937]\n",
      "loss: 1.692550 [mini-batch 500 / 937]\n",
      "loss: 1.852947 [mini-batch 600 / 937]\n",
      "loss: 2.014439 [mini-batch 700 / 937]\n",
      "loss: 2.044812 [mini-batch 800 / 937]\n",
      "loss: 1.712328 [mini-batch 900 / 937]\n",
      "==> Accuracy: 82.9%, Avg loss: 0.498303\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "loss: 1.527667 [mini-batch 0 / 937]\n",
      "loss: 1.874819 [mini-batch 100 / 937]\n",
      "loss: 1.377257 [mini-batch 200 / 937]\n",
      "loss: 1.825762 [mini-batch 300 / 937]\n",
      "loss: 2.062632 [mini-batch 400 / 937]\n",
      "loss: 1.655827 [mini-batch 500 / 937]\n",
      "loss: 1.813104 [mini-batch 600 / 937]\n",
      "loss: 1.926461 [mini-batch 700 / 937]\n",
      "loss: 2.000309 [mini-batch 800 / 937]\n",
      "loss: 1.736095 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.1%, Avg loss: 0.493192\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "loss: 1.531702 [mini-batch 0 / 937]\n",
      "loss: 1.837803 [mini-batch 100 / 937]\n",
      "loss: 1.264967 [mini-batch 200 / 937]\n",
      "loss: 1.801973 [mini-batch 300 / 937]\n",
      "loss: 1.963224 [mini-batch 400 / 937]\n",
      "loss: 1.629481 [mini-batch 500 / 937]\n",
      "loss: 1.729594 [mini-batch 600 / 937]\n",
      "loss: 1.842073 [mini-batch 700 / 937]\n",
      "loss: 1.987749 [mini-batch 800 / 937]\n",
      "loss: 1.772815 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.2%, Avg loss: 0.489926\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "loss: 1.563967 [mini-batch 0 / 937]\n",
      "loss: 1.789814 [mini-batch 100 / 937]\n",
      "loss: 1.149288 [mini-batch 200 / 937]\n",
      "loss: 1.760420 [mini-batch 300 / 937]\n",
      "loss: 1.938454 [mini-batch 400 / 937]\n",
      "loss: 1.612702 [mini-batch 500 / 937]\n",
      "loss: 1.648437 [mini-batch 600 / 937]\n",
      "loss: 1.778103 [mini-batch 700 / 937]\n",
      "loss: 1.954117 [mini-batch 800 / 937]\n",
      "loss: 1.808313 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.3%, Avg loss: 0.488219\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "loss: 1.564843 [mini-batch 0 / 937]\n",
      "loss: 1.731472 [mini-batch 100 / 937]\n",
      "loss: 1.101113 [mini-batch 200 / 937]\n",
      "loss: 1.741353 [mini-batch 300 / 937]\n",
      "loss: 1.864373 [mini-batch 400 / 937]\n",
      "loss: 1.579948 [mini-batch 500 / 937]\n",
      "loss: 1.601178 [mini-batch 600 / 937]\n",
      "loss: 1.741009 [mini-batch 700 / 937]\n",
      "loss: 1.895935 [mini-batch 800 / 937]\n",
      "loss: 1.835201 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.5%, Avg loss: 0.486599\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "loss: 1.528997 [mini-batch 0 / 937]\n",
      "loss: 1.685499 [mini-batch 100 / 937]\n",
      "loss: 1.086473 [mini-batch 200 / 937]\n",
      "loss: 1.715346 [mini-batch 300 / 937]\n",
      "loss: 1.786711 [mini-batch 400 / 937]\n",
      "loss: 1.524481 [mini-batch 500 / 937]\n",
      "loss: 1.567480 [mini-batch 600 / 937]\n",
      "loss: 1.702416 [mini-batch 700 / 937]\n",
      "loss: 1.854482 [mini-batch 800 / 937]\n",
      "loss: 1.829880 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.6%, Avg loss: 0.484620\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "loss: 1.511344 [mini-batch 0 / 937]\n",
      "loss: 1.649560 [mini-batch 100 / 937]\n",
      "loss: 1.089824 [mini-batch 200 / 937]\n",
      "loss: 1.683579 [mini-batch 300 / 937]\n",
      "loss: 1.685130 [mini-batch 400 / 937]\n",
      "loss: 1.468458 [mini-batch 500 / 937]\n",
      "loss: 1.543932 [mini-batch 600 / 937]\n",
      "loss: 1.673437 [mini-batch 700 / 937]\n",
      "loss: 1.829129 [mini-batch 800 / 937]\n",
      "loss: 1.805295 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.7%, Avg loss: 0.483913\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "loss: 1.499550 [mini-batch 0 / 937]\n",
      "loss: 1.625659 [mini-batch 100 / 937]\n",
      "loss: 1.076551 [mini-batch 200 / 937]\n",
      "loss: 1.660194 [mini-batch 300 / 937]\n",
      "loss: 1.608260 [mini-batch 400 / 937]\n",
      "loss: 1.418267 [mini-batch 500 / 937]\n",
      "loss: 1.508911 [mini-batch 600 / 937]\n",
      "loss: 1.661709 [mini-batch 700 / 937]\n",
      "loss: 1.824736 [mini-batch 800 / 937]\n",
      "loss: 1.768705 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.8%, Avg loss: 0.482504\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "loss: 1.505293 [mini-batch 0 / 937]\n",
      "loss: 1.625242 [mini-batch 100 / 937]\n",
      "loss: 1.051745 [mini-batch 200 / 937]\n",
      "loss: 1.636515 [mini-batch 300 / 937]\n",
      "loss: 1.562691 [mini-batch 400 / 937]\n",
      "loss: 1.395935 [mini-batch 500 / 937]\n",
      "loss: 1.486798 [mini-batch 600 / 937]\n",
      "loss: 1.646850 [mini-batch 700 / 937]\n",
      "loss: 1.861922 [mini-batch 800 / 937]\n",
      "loss: 1.720371 [mini-batch 900 / 937]\n",
      "==> Accuracy: 84.0%, Avg loss: 0.481017\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "loss: 1.503152 [mini-batch 0 / 937]\n",
      "loss: 1.661804 [mini-batch 100 / 937]\n",
      "loss: 1.040662 [mini-batch 200 / 937]\n",
      "loss: 1.629507 [mini-batch 300 / 937]\n",
      "loss: 1.549721 [mini-batch 400 / 937]\n",
      "loss: 1.379823 [mini-batch 500 / 937]\n",
      "loss: 1.458041 [mini-batch 600 / 937]\n",
      "loss: 1.632492 [mini-batch 700 / 937]\n",
      "loss: 1.891390 [mini-batch 800 / 937]\n",
      "loss: 1.684113 [mini-batch 900 / 937]\n",
      "==> Accuracy: 84.0%, Avg loss: 0.481199\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "loss: 1.489201 [mini-batch 0 / 937]\n",
      "loss: 1.688897 [mini-batch 100 / 937]\n",
      "loss: 1.033139 [mini-batch 200 / 937]\n",
      "loss: 1.612654 [mini-batch 300 / 937]\n",
      "loss: 1.549743 [mini-batch 400 / 937]\n",
      "loss: 1.350263 [mini-batch 500 / 937]\n",
      "loss: 1.450286 [mini-batch 600 / 937]\n",
      "loss: 1.614182 [mini-batch 700 / 937]\n",
      "loss: 1.905769 [mini-batch 800 / 937]\n",
      "loss: 1.662432 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.9%, Avg loss: 0.482327\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "loss: 1.492708 [mini-batch 0 / 937]\n",
      "loss: 1.700057 [mini-batch 100 / 937]\n",
      "loss: 1.033863 [mini-batch 200 / 937]\n",
      "loss: 1.611727 [mini-batch 300 / 937]\n",
      "loss: 1.546313 [mini-batch 400 / 937]\n",
      "loss: 1.313835 [mini-batch 500 / 937]\n",
      "loss: 1.463386 [mini-batch 600 / 937]\n",
      "loss: 1.622423 [mini-batch 700 / 937]\n",
      "loss: 1.900354 [mini-batch 800 / 937]\n",
      "loss: 1.664946 [mini-batch 900 / 937]\n",
      "==> Accuracy: 83.9%, Avg loss: 0.482263\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "loss: 1.491206 [mini-batch 0 / 937]\n",
      "loss: 1.681659 [mini-batch 100 / 937]\n",
      "loss: 1.035693 [mini-batch 200 / 937]\n",
      "loss: 1.621457 [mini-batch 300 / 937]\n",
      "loss: 1.526303 [mini-batch 400 / 937]\n",
      "loss: 1.292015 [mini-batch 500 / 937]\n",
      "loss: 1.487636 [mini-batch 600 / 937]\n",
      "loss: 1.636891 [mini-batch 700 / 937]\n",
      "loss: 1.892581 [mini-batch 800 / 937]\n",
      "loss: 1.669192 [mini-batch 900 / 937]\n",
      "==> Accuracy: 84.0%, Avg loss: 0.479471\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "loss: 1.458275 [mini-batch 0 / 937]\n",
      "loss: 1.657359 [mini-batch 100 / 937]\n",
      "loss: 1.022142 [mini-batch 200 / 937]\n",
      "loss: 1.628818 [mini-batch 300 / 937]\n",
      "loss: 1.519919 [mini-batch 400 / 937]\n",
      "loss: 1.277984 [mini-batch 500 / 937]\n",
      "loss: 1.508044 [mini-batch 600 / 937]\n",
      "loss: 1.658138 [mini-batch 700 / 937]\n",
      "loss: 1.878805 [mini-batch 800 / 937]\n",
      "loss: 1.687479 [mini-batch 900 / 937]\n",
      "==> Accuracy: 84.0%, Avg loss: 0.476811\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "loss: 1.425708 [mini-batch 0 / 937]\n",
      "loss: 1.632968 [mini-batch 100 / 937]\n",
      "loss: 1.019417 [mini-batch 200 / 937]\n",
      "loss: 1.650451 [mini-batch 300 / 937]\n",
      "loss: 1.497456 [mini-batch 400 / 937]\n",
      "loss: 1.271274 [mini-batch 500 / 937]\n",
      "loss: 1.528675 [mini-batch 600 / 937]\n",
      "loss: 1.668255 [mini-batch 700 / 937]\n",
      "loss: 1.852049 [mini-batch 800 / 937]\n",
      "loss: 1.683590 [mini-batch 900 / 937]\n",
      "==> Accuracy: 84.0%, Avg loss: 0.474414\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "loss: 1.391450 [mini-batch 0 / 937]\n",
      "loss: 1.624250 [mini-batch 100 / 937]\n",
      "loss: 1.030518 [mini-batch 200 / 937]\n",
      "loss: 1.671348 [mini-batch 300 / 937]\n",
      "loss: 1.475321 [mini-batch 400 / 937]\n",
      "loss: 1.263717 [mini-batch 500 / 937]\n",
      "loss: 1.566438 [mini-batch 600 / 937]\n",
      "loss: 1.664515 [mini-batch 700 / 937]\n",
      "loss: 1.833550 [mini-batch 800 / 937]\n",
      "loss: 1.669880 [mini-batch 900 / 937]\n",
      "==> Accuracy: 84.1%, Avg loss: 0.473049\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "loss: 1.358271 [mini-batch 0 / 937]\n",
      "loss: 1.627482 [mini-batch 100 / 937]\n",
      "loss: 1.045792 [mini-batch 200 / 937]\n",
      "loss: 1.693730 [mini-batch 300 / 937]\n",
      "loss: 1.453199 [mini-batch 400 / 937]\n",
      "loss: 1.267366 [mini-batch 500 / 937]\n",
      "loss: 1.602061 [mini-batch 600 / 937]\n",
      "loss: 1.669712 [mini-batch 700 / 937]\n",
      "loss: 1.841729 [mini-batch 800 / 937]\n",
      "loss: 1.651745 [mini-batch 900 / 937]\n",
      "==> Accuracy: 84.2%, Avg loss: 0.471497\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performance baseline\n",
    "\n",
    "print(\"Pre-train stats\")\n",
    "accuracy, avg_loss = test()\n",
    "print(f\"==> Accuracy: {100*accuracy:0.1f}%, Avg loss: {avg_loss:>8f}\\n\")\n",
    "\n",
    "\n",
    "# Training loop\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    print(f\"Epoch {epoch}\\n\" + 20*'-')\n",
    "    rng.shuffle(training_index)\n",
    "    for mini_batch_number in range(n_mini_batches):\n",
    "        sum_nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "        sum_nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "        sum_loss = 0\n",
    "        mini_batch = train_data[mini_batch_number*mini_batch_size : (mini_batch_number+1)*mini_batch_size]\n",
    "        for x, y in mini_batch:\n",
    "            nabla_b, nabla_w, loss = calc_gradient(x, y)\n",
    "            for l in range(num_layers):\n",
    "                sum_nabla_b[l] += nabla_b[l]\n",
    "                sum_nabla_w[l] += nabla_w[l]\n",
    "                sum_loss += loss\n",
    "        n = len(mini_batch)\n",
    "        for l in range(num_layers):\n",
    "            biases[l] -= learning_rate * sum_nabla_b[l] / n\n",
    "            weights[l] -= learning_rate * sum_nabla_w[l] / n\n",
    "        if mini_batch_number % 100 == 0:\n",
    "            print(f\"loss: {sum_loss/n:>8f} [mini-batch {mini_batch_number} / {n_mini_batches}]\")\n",
    "    # Test at the end of each epoch\n",
    "    accuracy, avg_loss = test()\n",
    "    print(f\"==> Accuracy: {100*accuracy:0.1f}%, Avg loss: {avg_loss:>8f}\\n\")        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
