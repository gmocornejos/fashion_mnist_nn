{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd830ba-9614-41f4-b21e-1e3204cf750f",
   "metadata": {},
   "source": [
    "# Math preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed57870-ca6b-48da-bf03-0daf6c19217d",
   "metadata": {},
   "source": [
    "### Chain rule\n",
    "\n",
    "**Composition of functions:**\n",
    "\n",
    "$$\n",
    "\\cfrac{d f(g(x))}{dx} = \\cfrac{d f}{dx} (g(x)) \\cfrac{d g(x)}{dx}\n",
    "$$\n",
    "\n",
    "This means we derivate $f$, and evaluate the resulting function at $g(x)$. Then, we derivate $g$, and evaluate the resulting function at $x$. Finally, we multiply both.\n",
    "\n",
    "**With a third dependent variable:**\n",
    "\n",
    "$$\n",
    "\\cfrac{dz}{dx} = \\cfrac{dz}{dy} \\cdot \\cfrac{dy}{dx}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80108e47-aa6a-45f4-9d7a-2b35c64c9e68",
   "metadata": {},
   "source": [
    "### Quotient rule\n",
    "\n",
    "$$\n",
    "\\cfrac{df(x)}{dg(x)} = \\cfrac{\\cfrac{df(x)}{dx} \\cdot g(x) - f(x) \\cdot \\cfrac{dg(x)}{dx}}{(g(x))^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809ff8a-2356-41de-9e99-b96628adfb81",
   "metadata": {},
   "source": [
    "### Nabla notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966e695-0265-48fa-8c5f-ded40eae7987",
   "metadata": {},
   "source": [
    "### Kronecker delta function\n",
    "\n",
    "$$\n",
    "d_{i, j} = \n",
    "    \\begin{cases}\n",
    "        0 \\quad\\text{if} ~ i \\neq j \\\\\n",
    "        1 \\quad\\text{if} ~ i = j\n",
    "    \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24055f-c9dc-4c6b-8c4b-4905ffecefbc",
   "metadata": {},
   "source": [
    "### Cross-entropy loss\n",
    "\n",
    "$$\n",
    "L = - \\sum_{k=0}^{n_L} y_k ~ log(a^L_k)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "* $k_y$ is the k-th entry of the one-hot encoded class indicator, $\\mathbf{y}$\n",
    "* $a_k^L$ is the activation of unit $k$ in the output layer (numbered as $L$)\n",
    "* $n_L$ is the number of units at layer $L$ (the output layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc8d76-fe3d-4502-82dd-89f966781d7c",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "$$\n",
    "Softmax(k, L)=\\cfrac{e^{z_k^L}}{\\sum_{j=0}^{n_L} e^{z_j^L}}\n",
    "$$\n",
    "\n",
    "Where\n",
    "* $z_k^L$ is the logit of unit $k$ at layer $L$\n",
    "* $z_j^L$ is the logit of unit $j$ at layer $L$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6af4d-528b-4704-8466-834148f00302",
   "metadata": {},
   "source": [
    "### ReLU activation function\n",
    "\n",
    "$$\n",
    "ReLU(x) = \\begin{cases}\n",
    "    0 \\quad \\text{if} ~ x \\leq 0 \\\\\n",
    "    x \\quad \\text{if} ~ x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This functions is commonly implemented as $max(0, x)$. The derivative of ReLU is also function in parts\n",
    "\n",
    "$$\n",
    "\\cfrac{d ~ ReLU(x)}{dx} = ReLU'(x) = \\begin{cases}\n",
    "    0 \\quad \\text{if} ~ x = 0 \\\\\n",
    "    1 \\quad \\text{if} ~ x \\neq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5c1163-b985-481a-80bf-112970a71f7d",
   "metadata": {},
   "source": [
    "### Hadamard product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b643546b-a755-402c-b318-5ff3c2c8b0fc",
   "metadata": {},
   "source": [
    "### Outer product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67adb6b9-b849-4e7b-89c4-a1316e41746c",
   "metadata": {},
   "source": [
    "# Training the last layer\n",
    "\n",
    "Let's say we want to decrease the loss by modifying just a single weight from the last layer (layer $L$). We can use gradient descent for that. The first thing we need to know the how the Loss changes with respect to that weight, applying the chain rule, we split that into two partial derivaties that are easier to calculate\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial Loss}{\\partial w_{ij}^L} = \\cfrac{\\partial Loss}{\\partial z_i^L} \\cfrac{\\partial z_i^L}{\\partial w_{ij}^L}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e14ae-0260-4b6b-8899-736c333a84a0",
   "metadata": {},
   "source": [
    "### $\\partial Loss / \\partial z_i^L$\n",
    "\n",
    "Let's consider the effect of a small change in the output from unit $i$ at the last layer ($L$) over the loss. $z_i^L$ is the output (the \"logit\") of unit $i$ in the output (final) layer $L$. Replacing the Loss with the definition of the cross-entropy we get:\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial Loss}{\\partial z^L_i} = \\cfrac{\\partial}{\\partial z_i^L} [- \\sum_{k=0}^{n_L} y_k ~ log(a_k^L)]\n",
    "$$\n",
    "\n",
    "\n",
    "The sum and the partial derivative are linear with repect to each other, so, we can move the derivative into the sum\n",
    "\n",
    "$$\n",
    "= - \\sum_{k=0}^{n_L} y_k ~ \\cfrac{\\partial}{\\partial z_i^L} [log(a_k^L)]\n",
    "$$\n",
    "\n",
    "\n",
    "Following the chain rule for the composition of functions:\n",
    "\n",
    "$$\n",
    "= - \\sum_{k=0}^{n_L} y_k \\cdot \\cfrac{1}{a_k^L} \\cdot \\cfrac{\\partial a_k^L}{\\partial z_i^L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d2ed3-f254-4554-9d55-4f0986741ec4",
   "metadata": {},
   "source": [
    "Let's work out the remaining partial derivate. This is, how a small change in the logit of unit $i$ changes the activation of unit $k$. The activation at the last layer is produced by taking the softmax function over the logits:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\cfrac{\\partial a_k^L}{\\partial z_k^L} &= \\cfrac{\\partial}{\\partial z_i^L} Softmax(k, L) \\\\\n",
    "        &= \\cfrac{\\partial}{\\partial z_i^L} [\\cfrac{e^{z_k^L}}{\\sum_{j=0}^{n_L} e^{z_j^L}}]\n",
    "\\end{align}        \n",
    "$$\n",
    "\n",
    "Applying the Quotient rule:\n",
    "\n",
    "$$\n",
    "= \\cfrac{(\\cfrac{\\partial e^{z_k^L}}{\\partial z_i^L} \\cdot \\sum_{j=0}^{n_L} e^{z_j^L}) - (e^{z_k^L} \\cdot \\cfrac{\\partial}{\\partial z_i^L} \\sum_{j=0}^{n_L} e^{z_j^L} )}{(\\sum_{j=0}^{n_L} e^{z_j^L})^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f693351-cd9b-471b-8db8-332823b288f7",
   "metadata": {},
   "source": [
    "Again, let's consider each derivative individually. For the first one, notice that if $k = i$, then\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial e^{z_i^L}}{\\partial z_i^L} = e^{z_i^L}\n",
    "$$\n",
    "\n",
    "Otherwise, if $k \\neq i$, then \n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial e^{z_k^L}}{\\partial z_i^L} = 0\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial e^{z_k^L}}{\\partial z_i^L} = \\delta_{k, i} e^{z_i^L}\n",
    "$$\n",
    "\n",
    "For the second one, the can pull the partial derivative into the sum\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial}{\\partial z_i^L} \\sum_{j=0}^{n^L} e^{z_j^L} = \\sum_{j=0}^{n_L} \\cfrac{\\partial}{\\partial z_i^L} e^{z_j^L}\n",
    "$$\n",
    "\n",
    "Similarly to the derivative above, all the terms for which $j \\neq i$ are zero, hence\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial}{\\partial z_i^L} \\sum_{j=0}^{n^L} e^{z_j^L} = e^{z_i^L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b014b88-d279-44a2-af30-ce58482ea3e7",
   "metadata": {},
   "source": [
    "Plugging both derivatives back:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\cfrac{\\partial a_k^L}{\\partial z_k^L} &= \\cfrac{(\\delta_{k, i} e^{z_i^L} \\cdot \\sum_{j=0}^{n_L} e^{z_j^L}) - (e^{z_k^L} \\cdot e^{z_i^k}) }{(\\sum_{j=0}^{n_L} e^{z_j^L})^2} \\\\\n",
    "        &= \\cfrac{e^{z_i^L}}{\\sum_{j=0}^{n_L} e^{z_j^L} } \\cdot \\cfrac{\\delta_{k, i} \\sum_{j=0}^{n_L} e^{z_j^L} - e^{z_k^L}}{\\sum_{j=0}^{n_L} e^{z_j^L}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The last step was just a little bit of algebraic re-arragement (factor our $e^{z_i^L}$, split into two fractions). Now, hopefully you can see that the first fraction is the definition of the $Softmax$ function. Also, we can split the second faction at the minus, the sums cancel each other, that leaves the delta only, and at the right yet another definition of the $Softmax$. In both instances, remember that the $a_j^L = Softmax(j, L)$.\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial a_k^L}{\\partial z_k^L} = a_i^L (\\delta_{k, i} - a_k^L)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6a8081-28a7-4206-8ea8-be5caa18e3ae",
   "metadata": {},
   "source": [
    "Finally, plugging that derivative back into the initial expression:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\cfrac{\\partial Loss}{\\partial z^L_i} &= - \\sum_{k=0}^{n_L} y_k \\cdot \\cfrac{1}{a_k^L} \\cdot \\cfrac{\\partial a_k^L}{\\partial z_i^L} \\\\\n",
    "    &= - \\sum_{k=0}^{n_L} y_k \\cdot \\cfrac{1}{a_k^L} \\cdot a_i^L (\\delta_{k, i} - a_k^L)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe6e0ea-a795-451f-b846-1d85c99c9097",
   "metadata": {},
   "source": [
    "Notice that we can \"extract\" the case $k = i$ from the summation\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "= - (y_i \\cdot \\cfrac{1}{a_i^L} \\cdot a_i^L (\\delta_{i, i} - a_i^L)) &- \\sum_{k=0, k \\neq i}^{n_L} y_k \\cdot \\cfrac{1}{a_k^L} \\cdot a_i^L (\\delta_{k, i} - a_k^L)\\\\\n",
    "= - y_i (1 - a_i^L) &- \\sum_{k=0, k \\neq i}^{n_L} y_k \\cdot \\cfrac{1}{a_k^L} \\cdot a_i^L (0 - a_k^L) \\\\\n",
    "= - y_i + y_i a_i^L &- \\sum_{k=0, k \\neq i}^{n_L} y_k \\cdot - a_i^L \\\\\n",
    "= - y_i + a_i^L y_i &+ a_i^L \\sum_{k=0, k \\neq i}^{n_L} y_k \\\\\n",
    "= - y_i + a_i^L & (y_i + \\sum_{k=0, k \\neq i}^{n_L} y_k) \\\\\n",
    "= - y_i &+ a_i^L \\sum_{k=0}^{n_L} y_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the last step, we added the $k=i$ case back into the sum by absorbing $y_i$. Now, $\\mathbf{y}$ is a one-hot encoded vector, therefore $\\sum_{k=0}^{n_L} y_k = 1$:\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial Loss}{\\partial z^L_i} = a_i^L - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3a54e9-dce4-48d8-9de7-ee8b821c71ac",
   "metadata": {},
   "source": [
    "### $\\partial z_i^L / \\partial w_{ji}^L$\n",
    "\n",
    "The logit of a unit is calculated by multiplying the previous layer activation by the weights matrix, then adding the biases. Substituting the definition we get:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\cfrac{\\partial z_i^L}{\\partial w_{ij}^L} &= \\cfrac{\\partial}{\\partial w_{ij}^L} (\\sum_{k=0}^{n_L} w_{ik}^{L} a_k^{L-1} + b_k^L) \\\\\n",
    "    &= \\sum_{k=0}^{n_L} \\cfrac{\\partial}{\\partial w_{ij}^L} (w_{ik}^{L} a_k^{L-1} + b_k^L) \\\\\n",
    "    &= a_j^{L-1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As in other cases above, the partial derivate evaluates to zero for all cases except $k = j$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d18d8-97a5-4fa8-a4d5-e75597cd5af4",
   "metadata": {},
   "source": [
    "## $\\partial Loss / \\partial w_{ji}^L$\n",
    "\n",
    "Finally!\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\cfrac{\\partial Loss}{\\partial w_{ij}^L} &= \\cfrac{\\partial Loss}{\\partial z_i^L} \\cfrac{\\partial z_i^L}{\\partial w_{ij}^L} \\\\\n",
    "        &= (a_i^L - y_i) ~ a_j^{L-1} \\\\\n",
    "        &= a_j^{L-1} ~ (a_i^L - y_i) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92173751-443d-4a1e-b0cf-0f1c3b788163",
   "metadata": {},
   "source": [
    "We've been using index notation, but in Numpy, we work with vectors and matrices. So, let's transform that expression into matrix form. To make it a little easier to see the correspondance between both representations, we can plug some actual indices there. For intance, input indices $j = {0, 1, 2, 3}$ and output indices $i = {0, 1, 2}$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\cfrac{\\partial Loss}{\\partial w_{00}^L} & \\cfrac{\\partial Loss}{\\partial w_{01}^L} & \\cfrac{\\partial Loss}{\\partial w_{02}^L} \\\\\n",
    "    \\cfrac{\\partial Loss}{\\partial w_{10}^L} & \\cfrac{\\partial Loss}{\\partial w_{11}^L} & \\cfrac{\\partial Loss}{\\partial w_{12}^L} \\\\\n",
    "    \\cfrac{\\partial Loss}{\\partial w_{20}^L} & \\cfrac{\\partial Loss}{\\partial w_{21}^L} & \\cfrac{\\partial Loss}{\\partial w_{22}^L} \\\\\n",
    "    \\cfrac{\\partial Loss}{\\partial w_{30}^L} & \\cfrac{\\partial Loss}{\\partial w_{31}^L} & \\cfrac{\\partial Loss}{\\partial w_{32}^L} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f51842e-2256-4be9-bd6e-1d329b0e7507",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\begin{bmatrix}\n",
    "    (a_0^L - y_0) a_0^{L-1} & (a_0^L - y_0) a_1^{L-1} & (a_0^L - y_0) a_2^{L-1} \\\\\n",
    "    (a_1^L - y_1) a_0^{L-1} & (a_1^L - y_1) a_1^{L-1} & (a_1^L - y_1) a_2^{L-1} \\\\\n",
    "    (a_2^L - y_2) a_0^{L-1} & (a_2^L - y_2) a_1^{L-1} & (a_2^L - y_2) a_2^{L-1} \\\\\n",
    "    (a_3^L - y_3) a_0^{L-1} & (a_3^L - y_3) a_1^{L-1} & (a_3^L - y_3) a_2^{L-1} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\\begin{bmatrix}\n",
    "    (a_0^L - y_0) \\\\\n",
    "    (a_1^L - y_1) \\\\\n",
    "    (a_2^L - y_2) \\\\\n",
    "    (a_3^L - y_3) \\\\\n",
    "\\end{bmatrix}\n",
    "\\otimes\n",
    "\\begin{bmatrix}\n",
    "    a_0^{L-1} \\\\\n",
    "    a_1^{L-1} \\\\\n",
    "    a_2^{L-1} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a9722a-29f8-45f1-8cf4-aef5f67c3978",
   "metadata": {},
   "source": [
    "In full matrix form:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{W^L}} L =  (\\mathbf{a^L} - \\mathbf{y}) \\otimes \\mathbf{a^{L-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79338b-1b20-4648-9a6f-4b6ce6395a2a",
   "metadata": {},
   "source": [
    "### What about the biases?\n",
    "\n",
    "Actually, we're almost done there:\n",
    "\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial Loss}{\\partial b_{i}^L} = \\cfrac{\\partial Loss}{\\partial z_i^L} \\cfrac{\\partial z_i^L}{\\partial b_{i}^L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd1b31-24a3-4a3f-9475-a0a2dc4e6d37",
   "metadata": {},
   "source": [
    "We just have to figure out $\\partial z_i^L / \\partial b_i^L$. Again, if we substitute with the definition for logits:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\cfrac{\\partial z_i^L}{\\partial b_i^L} &= \\cfrac{\\partial}{\\partial b_i^L} (\\sum_{k=0}^{n_L} w_{kj}^{L} a_k^{L-1} + b_k^L) \\\\\n",
    "    &= \\sum_{k=0}^{n_L} \\cfrac{\\partial}{\\partial w_{ij}^L} (\\sum_{k=0}^{n_L} w_{kj}^{L} a_k^{L-1} + b_k^L) \\\\\n",
    "    &= 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For all cases when $k \\neq i$ the derivative is zero, for $k = i$ it's 1. Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c8face-2d21-49ca-ab97-41fcfe93b828",
   "metadata": {},
   "source": [
    "$$\n",
    "\\cfrac{\\partial Loss}{\\partial b_{i}^L} = \\cfrac{\\partial Loss}{\\partial z_i^L} \\cfrac{\\partial z_i^L}{\\partial b_{i}^L} = a_i^L - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0f735-d624-4eac-a9a1-f5694e86b52a",
   "metadata": {},
   "source": [
    "In matrix form: \n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{b^L}} L = \\mathbf{a^L} - \\mathbf{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e742f-c9e4-4764-9e81-94fe631d179e",
   "metadata": {},
   "source": [
    "# Training the other layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcc3d4f-4b11-4652-b9ad-a9532d04a56e",
   "metadata": {},
   "source": [
    "As ussual, let's apply the chain rule:\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial L}{\\partial w_{ji}^{L-1}} = \\cfrac{\\partial L}{\\partial z_i^{L-1}} \\cfrac{\\partial z_i^{L-1}}{\\partial w_{ji}^{L-1}}\n",
    "$$\n",
    "\n",
    "Or in recursive form:\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial L}{\\partial w_{ji}^{l}} = \\cfrac{\\partial L}{\\partial z_i^{l}} \\cfrac{\\partial z_i^{l}}{\\partial w_{ji}^{l}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc8c6be-1c95-4dd8-ad22-23d309e64eb6",
   "metadata": {},
   "source": [
    "### $\\partial z_i^{l} / \\partial w_{ji}^l$\n",
    "\n",
    "\n",
    "There is nothing special about the result from the last layer:\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial z_i^{L}}{\\partial w_{ij}^L} = a_j^{L-1}\n",
    "$$\n",
    "\n",
    "it can be written directly into a recursive form\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial z_i^{l}}{\\partial w_{ij}^l} = a_j^{l-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8850fb6-9860-499e-b52b-6b36ae2e7983",
   "metadata": {},
   "source": [
    "### $\\partial L / \\partial z_i^{l}$\n",
    "\n",
    "A little change in the loss due to a change in a particular logit at layer $L-1$ is mediated by a change in **all** logits in layer $L$\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial L}{\\partial z_i^{L-1}} = \\sum_{k=0}^{n_L} \\cfrac{\\partial L}{\\partial z_k^L} \\cfrac{\\partial z_k^L}{\\partial z_i^{L-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc9fb20-eedf-4c74-8040-43fe9c48cef4",
   "metadata": {},
   "source": [
    "We already know $\\partial L / \\partial z_k^L$, we can focus on the second derivative. Substituting the definition of the logit, then the definition of the activation:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\cfrac{\\partial z_k^L}{\\partial z_i^{L-1}} &= \\cfrac{\\partial}{\\partial z_i^{L-1}} (\\sum_{j=0}^{n_L} w_{kj}^L a_j^{L-1} + b_j^L) \\\\\n",
    "        &= \\cfrac{\\partial}{\\partial z_i^{L-1}} (\\sum_{j=0}^{n_L} w_{kj}^L ReLU(z_j^{L-1}) + b_j^L) \\\\ \n",
    "        &= \\sum_{j=0}^{n_L} \\cfrac{\\partial}{\\partial z_i^{L-1}} (w_{kj}^L ReLU(z_j^{L-1}) + b_j^L) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931c5a6c-06c9-4283-b70e-8b68534c1630",
   "metadata": {},
   "source": [
    "As in other situations, the derivate is non zero only if $j = i$, then:\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial z_k^L}{\\partial z_i^{L-1}} = w_{ki}^L ReLU'(z_i^{L-1})\n",
    "$$\n",
    "\n",
    "Plugging that back, we get:\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial L}{\\partial z_i^{L-1}} = \\sum_{k=0}^{n_L} \\cfrac{\\partial L}{\\partial z_k^L} w_{ik}^L ReLU'(z_i^{L-1})\n",
    "$$\n",
    "\n",
    "And then back into the original expression:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\cfrac{\\partial L}{\\partial w_{ij}^{L-1}} &= [\\sum_{k=0}^{n_L} \\cfrac{\\partial L}{\\partial z_k^L} w_{ik}^L ReLU'(z_i^{L-1}) ] \\cfrac{\\partial z_i^{L-1}}{\\partial w_{ij}^{L-1}} \\\\\n",
    "        &= [ReLU'(z_i^{L-1}) \\sum_{k=0}^{n_L} \\cfrac{\\partial L}{\\partial z_k^L} w_{ik}^L] ~ a_j^{L-2}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae773c-989a-411f-84b7-57c0a9cfffba",
   "metadata": {},
   "source": [
    "We can transform that into a its recursive version\n",
    "\n",
    "$$\n",
    "    \\cfrac{\\partial L}{\\partial w_{ij}^{l}} = [ReLU'(z_i^{l}) \\sum_{k=0}^{n_{l+1}} \\cfrac{\\partial L}{\\partial z_k^{l+1}} w_{ik}^{l+1}] ~ a_j^{l-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed54bb8-0351-4373-bfee-68a655a71514",
   "metadata": {},
   "source": [
    "Notice that (yep, just swaping the terms)\n",
    "\n",
    "$$\n",
    "\\sum_{k=0}^{n_{l+1}} \\cfrac{\\partial L}{\\partial z_k^{l+1}} w_{ik}^{l+1} = \\sum_{k=0}^{n_{l+1}} w_{ik}^{l+1} \\cfrac{\\partial L}{\\partial z_k^{l+1}}\n",
    "$$\n",
    "\n",
    "Is just the regular matrix-vector multiplication. Introducing the nabla operator, we can rewrite \n",
    "\n",
    "$$\n",
    "\\sum_{k=0}^{n_{l+1}} \\cfrac{\\partial L}{\\partial z_k^{l+1}} w_{ik}^{l+1} = (\\mathbf{W}^{l+1})^T ~ \\nabla_{z^{l+1}} L \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b822d8fe-db5e-4017-97d7-fdef82890739",
   "metadata": {},
   "source": [
    "This help us to write the expression in matrix form:\n",
    "\n",
    "$$\n",
    "\\nabla_{W^l}L = [ReLU'(z^l) \\odot ((\\mathbf{W}^{l+1})^T \\nabla_{z^{l+1}} L)] \\otimes \\mathbf{a^{l-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0308ebb2-238f-4366-839a-ff725e44c93f",
   "metadata": {},
   "source": [
    "### What about the biases?\n",
    "\n",
    "Similarly to the weights\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial L}{\\partial b_i^{L-1}} = \\cfrac{\\partial L}{\\partial z_i^{L-1}} \\cfrac{\\partial z_i^{L-1}}{\\partial b_i^{L-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28908d-2fc8-4df8-9bde-4a8e361acf82",
   "metadata": {},
   "source": [
    "Again, there is nothing special about the last layer regarding how the logits are calculates, it's the same linear operation:\n",
    "\n",
    "$$\n",
    "\\cfrac{\\partial z_i^{L-1}}{\\partial b_i^{L-1}} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37f4c8-54c7-46cb-910b-6caeb1a8ad1d",
   "metadata": {},
   "source": [
    "And we just derived the expression for $\\partial L / \\partial z_i^l$, we can go ahead and claim that:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{b}^l} L = ReLU'(z^l) \\odot ((\\mathbf{W}^{l+1})^T \\nabla_{z^{l+1}} L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2243ae88-0c50-42f5-9ba9-e6f3f2a03d0d",
   "metadata": {},
   "source": [
    "# Update rules for any layer\n",
    "\n",
    "Ladies and Gentlemen, with you, the update rules any layer:\n",
    "\n",
    "$$\n",
    "\\mathbf{W^l} \\leftarrow \\mathbf{W^l} - \\eta \\nabla_{\\mathbf{W^l}} L \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{b^l} \\leftarrow \\mathbf{b^l} - \\eta \\nabla_{\\mathbf{b^l}} L\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
